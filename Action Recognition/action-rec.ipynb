{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow tensorflow-gpu opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion from the frame that OpenCV grabs\n",
    "    image.flags.writeable = False #sets the image writeable status to false\n",
    "\n",
    "    results = model.process(image) #makes the detection, where MediaPipe is actually detecting from the frame ---- that OpenCV grabs [making prediction]\n",
    "    \n",
    "    image.flags.writeable = True #sets the image to writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #color conversion back\n",
    "    return image, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(121,22,76), thickness = 2, circle_radius = 4),\n",
    "        mp_drawing.DrawingSpec(color=(80,44,121), thickness = 2, circle_radius = 2)\n",
    "        )\n",
    "\n",
    "    # mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    # mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#set mediapipe model - will make initial detection, and then track keypoints - setting the different values. We can change these values\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:   \n",
    "    while cap.isOpened():\n",
    "\n",
    "        #feed, looping every single frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #Make the detections using mediapipe_detection function\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "\n",
    "        #Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #show the frame!\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"results.pose_landmarks\" is formatted as such: \n",
    "landmark {\n",
    "    x: \"\"\n",
    "    y: \"\"\n",
    "    z: \"\"\n",
    "    visibility: \"\"\n",
    "}\n",
    "\n",
    "where \"results.pose_landmarks.landmark\" is unformatted array, but can access each of 33 with \"results.pose_landmarks.landmark[i].visibility/x/y/z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "\n",
    "    #this is only for tutorial - not necessary for pose recognition\n",
    "    # lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    # rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    # return np.concatenate([pose, lh, rh])\n",
    "    return pose\n",
    "    #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file directory may be useful ...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATING TRAINING DATA\n",
    "\n",
    "DATA_PATH = os.path.join(\"MP_DATA\")\n",
    "actions = np.array(['lunge','fleche','nothing'])\n",
    "\n",
    "#30 videos of data\n",
    "no_sequences = 50\n",
    "\n",
    "#30 frames of length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COLLECTING DATA NOW!! THIS IS ONLY FOR TUTORIAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#set mediapipe model - will make initial detection, and then track keypoints - setting the different values. We can change these values\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:   \n",
    "\n",
    "    for action in actions: \n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                #feed, looping every single frame\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                #Make the detections using mediapipe_detection function\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "\n",
    "                #Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "\n",
    "                #apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0, 255), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, '{} Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                               \n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, '{} Number {}'.format(action, sequence), (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences, labels = [],[]\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)), allow_pickle = True)\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Train the LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other networks: Many state of the art models currently use a number of CNN layers + LSTM layers. Use pre-trained mobile net followed by LSTM layers. However, using our training data, no where near same accuracy. \n",
    "\n",
    "Our network: MediaPipe Holistic + LSTM layers\n",
    "1) Less Data Required\n",
    "2) Faster to Train\n",
    "3) Faster Detections\n",
    "\n",
    "Using CNN Layer => a ton more data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir = log_dir)\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (30,132))) #30 frames, with 132 keypoints\n",
    "model.add(LSTM(128, return_sequences=True, activation = 'relu'))  \n",
    "model.add(LSTM(64, return_sequences=False, activation = 'relu'))  #because next layer is dense layer - Andrew Ng Deep Learning Specialization\n",
    "model.add(Dense(64,  activation = 'relu'))  \n",
    "model.add(Dense(32,  activation = 'relu'))  \n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = \"Adam\", loss = 'categorical_crossentropy', metrics = ['categorical_accuracy']) #need to use categorical crossentropy because multi-class classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "3/3 [==============================] - 4s 417ms/step - loss: 1.0967 - categorical_accuracy: 0.3333\n",
      "Epoch 2/110\n",
      "3/3 [==============================] - 1s 404ms/step - loss: 1.0947 - categorical_accuracy: 0.3556\n",
      "Epoch 3/110\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.9627 - categorical_accuracy: 0.5889\n",
      "Epoch 4/110\n",
      "3/3 [==============================] - 1s 381ms/step - loss: 0.9570 - categorical_accuracy: 0.6667\n",
      "Epoch 5/110\n",
      "3/3 [==============================] - 1s 407ms/step - loss: 0.8377 - categorical_accuracy: 0.5778\n",
      "Epoch 6/110\n",
      "3/3 [==============================] - 1s 365ms/step - loss: 0.6460 - categorical_accuracy: 0.7000\n",
      "Epoch 7/110\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 1.1872 - categorical_accuracy: 0.6778\n",
      "Epoch 8/110\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 0.8674 - categorical_accuracy: 0.7556\n",
      "Epoch 9/110\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 0.7868 - categorical_accuracy: 0.7444\n",
      "Epoch 10/110\n",
      "3/3 [==============================] - 1s 376ms/step - loss: 0.6795 - categorical_accuracy: 0.8778\n",
      "Epoch 11/110\n",
      "3/3 [==============================] - 1s 384ms/step - loss: 0.7331 - categorical_accuracy: 0.8000\n",
      "Epoch 12/110\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.5741 - categorical_accuracy: 0.9000\n",
      "Epoch 13/110\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 0.5360 - categorical_accuracy: 0.8444\n",
      "Epoch 14/110\n",
      "3/3 [==============================] - 1s 382ms/step - loss: 0.3675 - categorical_accuracy: 0.9000\n",
      "Epoch 15/110\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 0.2655 - categorical_accuracy: 0.8889\n",
      "Epoch 16/110\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 0.2661 - categorical_accuracy: 0.8778\n",
      "Epoch 17/110\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.1968 - categorical_accuracy: 0.9222\n",
      "Epoch 18/110\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 0.2861 - categorical_accuracy: 0.8889\n",
      "Epoch 19/110\n",
      "3/3 [==============================] - 1s 386ms/step - loss: 0.2604 - categorical_accuracy: 0.8889\n",
      "Epoch 20/110\n",
      "3/3 [==============================] - 1s 367ms/step - loss: 0.2537 - categorical_accuracy: 0.8889\n",
      "Epoch 21/110\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 0.1925 - categorical_accuracy: 0.9222\n",
      "Epoch 22/110\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 0.1841 - categorical_accuracy: 0.9556\n",
      "Epoch 23/110\n",
      "3/3 [==============================] - 1s 365ms/step - loss: 0.2332 - categorical_accuracy: 0.9000\n",
      "Epoch 24/110\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 0.1968 - categorical_accuracy: 0.9222\n",
      "Epoch 25/110\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 0.1990 - categorical_accuracy: 0.9444\n",
      "Epoch 26/110\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.1714 - categorical_accuracy: 0.9222\n",
      "Epoch 27/110\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.1766 - categorical_accuracy: 0.9222\n",
      "Epoch 28/110\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.1581 - categorical_accuracy: 0.9222\n",
      "Epoch 29/110\n",
      "3/3 [==============================] - 1s 375ms/step - loss: 0.1429 - categorical_accuracy: 0.9556\n",
      "Epoch 30/110\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.1639 - categorical_accuracy: 0.9333\n",
      "Epoch 31/110\n",
      "3/3 [==============================] - 1s 361ms/step - loss: 0.2568 - categorical_accuracy: 0.9000\n",
      "Epoch 32/110\n",
      "3/3 [==============================] - 1s 387ms/step - loss: 0.4970 - categorical_accuracy: 0.8333\n",
      "Epoch 33/110\n",
      "3/3 [==============================] - 1s 353ms/step - loss: 0.3676 - categorical_accuracy: 0.8667\n",
      "Epoch 34/110\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.2189 - categorical_accuracy: 0.9667\n",
      "Epoch 35/110\n",
      "3/3 [==============================] - 1s 421ms/step - loss: 0.1807 - categorical_accuracy: 0.9667\n",
      "Epoch 36/110\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 0.1644 - categorical_accuracy: 0.9444\n",
      "Epoch 37/110\n",
      "3/3 [==============================] - 1s 399ms/step - loss: 0.1547 - categorical_accuracy: 0.9444\n",
      "Epoch 38/110\n",
      "3/3 [==============================] - 1s 367ms/step - loss: 0.1410 - categorical_accuracy: 0.9444\n",
      "Epoch 39/110\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 0.1176 - categorical_accuracy: 0.9667\n",
      "Epoch 40/110\n",
      "3/3 [==============================] - 1s 395ms/step - loss: 0.1044 - categorical_accuracy: 0.9667\n",
      "Epoch 41/110\n",
      "3/3 [==============================] - 1s 385ms/step - loss: 0.6747 - categorical_accuracy: 0.9333\n",
      "Epoch 42/110\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 3.2758 - categorical_accuracy: 0.6889\n",
      "Epoch 43/110\n",
      "3/3 [==============================] - 1s 361ms/step - loss: 1.1962 - categorical_accuracy: 0.7444\n",
      "Epoch 44/110\n",
      "3/3 [==============================] - 1s 412ms/step - loss: 0.8120 - categorical_accuracy: 0.5667\n",
      "Epoch 45/110\n",
      "3/3 [==============================] - 1s 408ms/step - loss: 0.5344 - categorical_accuracy: 0.6333\n",
      "Epoch 46/110\n",
      "3/3 [==============================] - 1s 385ms/step - loss: 0.3876 - categorical_accuracy: 0.9000\n",
      "Epoch 47/110\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 0.3188 - categorical_accuracy: 0.8778\n",
      "Epoch 48/110\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 0.2839 - categorical_accuracy: 0.9111\n",
      "Epoch 49/110\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 0.3063 - categorical_accuracy: 0.9111\n",
      "Epoch 50/110\n",
      "3/3 [==============================] - 1s 388ms/step - loss: 0.3855 - categorical_accuracy: 0.8222\n",
      "Epoch 51/110\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 0.2384 - categorical_accuracy: 0.9222\n",
      "Epoch 52/110\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 0.2447 - categorical_accuracy: 0.9444\n",
      "Epoch 53/110\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.1735 - categorical_accuracy: 0.9444\n",
      "Epoch 54/110\n",
      "3/3 [==============================] - 1s 407ms/step - loss: 0.1416 - categorical_accuracy: 0.9222\n",
      "Epoch 55/110\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 0.1465 - categorical_accuracy: 0.9111\n",
      "Epoch 56/110\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 0.1309 - categorical_accuracy: 0.9444\n",
      "Epoch 57/110\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.1103 - categorical_accuracy: 0.9444\n",
      "Epoch 58/110\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 0.1076 - categorical_accuracy: 0.9556\n",
      "Epoch 59/110\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 0.0999 - categorical_accuracy: 0.9556\n",
      "Epoch 60/110\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.0781 - categorical_accuracy: 0.9556\n",
      "Epoch 61/110\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 0.0897 - categorical_accuracy: 0.9556\n",
      "Epoch 62/110\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 0.0892 - categorical_accuracy: 0.9556\n",
      "Epoch 63/110\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 0.0871 - categorical_accuracy: 0.9444\n",
      "Epoch 64/110\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 0.1046 - categorical_accuracy: 0.9444\n",
      "Epoch 65/110\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.1149 - categorical_accuracy: 0.9444\n",
      "Epoch 66/110\n",
      "3/3 [==============================] - 1s 383ms/step - loss: 0.1102 - categorical_accuracy: 0.9444\n",
      "Epoch 67/110\n",
      "3/3 [==============================] - 1s 346ms/step - loss: 0.1054 - categorical_accuracy: 0.9444\n",
      "Epoch 68/110\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 0.1048 - categorical_accuracy: 0.9444\n",
      "Epoch 69/110\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.1024 - categorical_accuracy: 0.9444\n",
      "Epoch 70/110\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.0949 - categorical_accuracy: 0.9556\n",
      "Epoch 71/110\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 0.0892 - categorical_accuracy: 0.9556\n",
      "Epoch 72/110\n",
      "3/3 [==============================] - 1s 389ms/step - loss: 0.0834 - categorical_accuracy: 0.9556\n",
      "Epoch 73/110\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 0.0783 - categorical_accuracy: 0.9667\n",
      "Epoch 74/110\n",
      "3/3 [==============================] - 1s 362ms/step - loss: 14.4138 - categorical_accuracy: 0.9333\n",
      "Epoch 75/110\n",
      "3/3 [==============================] - 1s 381ms/step - loss: 0.0809 - categorical_accuracy: 0.9556\n",
      "Epoch 76/110\n",
      "3/3 [==============================] - 1s 355ms/step - loss: 0.0798 - categorical_accuracy: 0.9667\n",
      "Epoch 77/110\n",
      "3/3 [==============================] - 1s 391ms/step - loss: 0.0926 - categorical_accuracy: 0.9667\n",
      "Epoch 78/110\n",
      "3/3 [==============================] - 1s 371ms/step - loss: 0.1013 - categorical_accuracy: 0.9667\n",
      "Epoch 79/110\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 0.0902 - categorical_accuracy: 0.9667\n",
      "Epoch 80/110\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 1.1279 - categorical_accuracy: 0.7889\n",
      "Epoch 81/110\n",
      "3/3 [==============================] - 1s 400ms/step - loss: 17.4167 - categorical_accuracy: 0.4889\n",
      "Epoch 82/110\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 5.2673 - categorical_accuracy: 0.4222\n",
      "Epoch 83/110\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 3.8306 - categorical_accuracy: 0.4111\n",
      "Epoch 84/110\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 230.2446 - categorical_accuracy: 0.3111\n",
      "Epoch 85/110\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 12.9256 - categorical_accuracy: 0.4000\n",
      "Epoch 86/110\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 22.9998 - categorical_accuracy: 0.3889\n",
      "Epoch 87/110\n",
      "3/3 [==============================] - 1s 371ms/step - loss: 9.2189 - categorical_accuracy: 0.4000\n",
      "Epoch 88/110\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 3.9874 - categorical_accuracy: 0.3778\n",
      "Epoch 89/110\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 1.8912 - categorical_accuracy: 0.4111\n",
      "Epoch 90/110\n",
      "3/3 [==============================] - 1s 392ms/step - loss: 1.2546 - categorical_accuracy: 0.3667\n",
      "Epoch 91/110\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 1.1155 - categorical_accuracy: 0.3778\n",
      "Epoch 92/110\n",
      "3/3 [==============================] - 1s 371ms/step - loss: 1.1175 - categorical_accuracy: 0.3556\n",
      "Epoch 93/110\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 1.0246 - categorical_accuracy: 0.4333\n",
      "Epoch 94/110\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 1.0962 - categorical_accuracy: 0.3889\n",
      "Epoch 95/110\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 1.0440 - categorical_accuracy: 0.3889\n",
      "Epoch 96/110\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 1.0220 - categorical_accuracy: 0.4111\n",
      "Epoch 97/110\n",
      "3/3 [==============================] - 1s 362ms/step - loss: 1.0180 - categorical_accuracy: 0.3889\n",
      "Epoch 98/110\n",
      "3/3 [==============================] - 1s 355ms/step - loss: 1.0042 - categorical_accuracy: 0.4333\n",
      "Epoch 99/110\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 0.9904 - categorical_accuracy: 0.4444\n",
      "Epoch 100/110\n",
      "3/3 [==============================] - 1s 367ms/step - loss: 0.9765 - categorical_accuracy: 0.4222\n",
      "Epoch 101/110\n",
      "3/3 [==============================] - 1s 367ms/step - loss: 0.9673 - categorical_accuracy: 0.4111\n",
      "Epoch 102/110\n",
      "3/3 [==============================] - 1s 389ms/step - loss: 0.9396 - categorical_accuracy: 0.4778\n",
      "Epoch 103/110\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 0.9062 - categorical_accuracy: 0.5889\n",
      "Epoch 104/110\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.9003 - categorical_accuracy: 0.5000\n",
      "Epoch 105/110\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 0.9161 - categorical_accuracy: 0.5222\n",
      "Epoch 106/110\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 0.9401 - categorical_accuracy: 0.4333\n",
      "Epoch 107/110\n",
      "3/3 [==============================] - 1s 388ms/step - loss: 1.1142 - categorical_accuracy: 0.2667\n",
      "Epoch 108/110\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 1.1106 - categorical_accuracy: 0.2778\n",
      "Epoch 109/110\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 1.0999 - categorical_accuracy: 0.2667\n",
      "Epoch 110/110\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 1.0963 - categorical_accuracy: 0.2778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29faa295bb0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 110, callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            50432     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204,995\n",
      "Trainable params: 204,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_test)\n",
    "# actions[np.argmax(res[4])]\n",
    "# actions[np.argmax(y_test[4])]\n",
    "# res is modeled as such. res = [0.1, 0.5, 0.4] s.t. each value is the probability of an action occurring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action_classifier.h5')\n",
    "model.built = True\n",
    "model.load_weights('action_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[39,  2],\n",
       "        [ 0, 19]],\n",
       "\n",
       "       [[35,  0],\n",
       "        [ 2, 23]],\n",
       "\n",
       "       [[44,  0],\n",
       "        [ 0, 16]]], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "yhat = model.predict(x_test)\n",
    "ytrue = np.argmax(y_test, axis = 1).tolist()\n",
    "yhat = np.argmax(yhat, axis = 1).tolist()\n",
    "\n",
    "multilabel_confusion_matrix(ytrue, yhat) #top left, bottom right are true positive, true negative\n",
    "# accuracy_score(ytrue, yhat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 2 23  0]\n",
      " [ 0  0 16]]\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define function to implement confusion matrix with normalization capability\n",
    "def plot_confusion_matrix(cm, classes, normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "y_pred_log = model.predict(x_test)\n",
    "y_pred_log = np.argmax(y_pred_log,axis=1)\n",
    "y_test1 = np.argmax(y_test, axis=1)\n",
    "cnf_rtree = confusion_matrix(y_test1, y_pred_log)\n",
    "\n",
    "print(cnf_rtree)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(4, 4, forward=True)\n",
    "plot_confusion_matrix(cnf_rtree, classes=['lunge','fleche','nothing'],\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test in Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.7 #confidence metric - render if higher than the threshold\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#set mediapipe model - will make initial detection, and then track keypoints - setting the different values. We can change these values\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:   \n",
    "    while cap.isOpened():\n",
    "\n",
    "        #feed, looping every single frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #Make the detections using mediapipe_detection function\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "\n",
    "        #Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "\n",
    "        #prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis = 0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "        \n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res): #check for stability over past 10 frames\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "        # cv2.rectangle(image, (0,0), (640,640), (245,117,16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        #show the frame!\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06181f1dd0bfb88186b362bc3d4abd51f3663396e4fcdb218b063a4b4b0ab83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
